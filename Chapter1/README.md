## 머신러닝이란?
- 머신러닝은 데이터로부터 학습하도록 컴퓨터를 프로그래밍 하는 것
- 일반적인 정의는, 명시적인 프로그래밍 없이 컴퓨터가 학습하는 능력을 갖추게 하는 연구 분야이다.
#
### 데이터 마이닝
- 머신러닝 기술을 적용해서 대용량의 데이터를 분석하면 겉으로는 보이지 않던 패턴을 발견할 수 있다.
- 가끔 예상치 못한 연관 관계나 새로운 추세가 발견되기도 해서 해당 문제를 더 잘 이해하도록 도와준다.
#
### 머신러닝의 이용했을 때, 뛰어난 분야?
- 기존 솔루션으로는 많은 수동 조정과 규칙이 필요한 문제
    - 하나의 머신러닝 모델이 코드를 간단하게 만들고 전통적인 방법보다 더 잘 수행되도록 할 수 있다.
- 전통적인 방식으로는 해결 방법이 없는 복잡한 문제
    - 가장 뛰어난 머신러닝 기법으로 해결 방법을 찾을 수 있다.
- 유동적인 환경
    - 머신러닝 시스템은 새로운 데이터에 적응할 수 있다.
- 복잡한 문제와 대량의 데이터에서 통찰 얻기

#
### 머신러닝 시스템의 종류
- 지도, 비지도, 준지도, 강화 학습
    - 사람의 감독하에 훈련하는 것인지 그렇지 않은 것인지
- 온라인 학습, 배치 학습
    - 실시간으로 점진적인 학습을 하는지 아닌지
- 사례 기반 학습과 모델 기반 학습
    - 단순하게 알고 있는 데이터 포인트와 새 데이터 포인트를 비교하는 것인지 아니면 과학자들이 하는 것처럼 훈련 데이터셋에서 패턴을 발견하여 예측 모델을 만드는지

#
### 지도 학습과 비지도 하습
- 지도 학습 (supervised learning)
    - 알고리즘에 주입하는 훈련 데이터에 레이블(label)이라는 원하는 답이 포함된다.
    - 분류(classification)가 전형적인 지도 학습, 스팸 필터가 좋은 예이다.
    - 회귀(regression)는 예측 변수(predictor variable)라 부르는 특성(feature)을 사용해 중고차 가격 같은 타깃(target) 수치를 예측하는 것이다.
    - 지도 학습 알고리즘
        - k-최근접 이웃 (k-nearest neighbors)
        - 선형 회귀 (linear regression)
        - 로지스틱 회귀 (logistic regression)
        - 서포트 벡터 머신 (support vector machine, SVM)
        - 결정 트리 (decision tree)와 랜덤 포레스트 (random forest)
        - 신경망 (neural networks)

- 비지도 학습 (unsupervised learning)
    - 훈련 데이터에 레이블이 없다.
    - 시스템이 아무런 도움 없이 학습해야 한다.
    - 비지도 학습 알고리즘
        - 군집 (clustering)
            - k-평균 (k-means)
            - DBSCAN
            - 계층 군집 분석 (hierarchical cluster analysis, HCA)
        - 이상치 탐지 (outlier detection)와 특이치 탐지 (novelty detection)
            - 원-클래스 (one-class SVM)
            - 아이솔레이션 포레스트 (isolation forest)
        - 시각화 (visualization)와 차원 축소 (dimensionality reduction)
            - 주성분 분석 (principal component analysis, PCA)
            - 커널 (kernel) PCA
            - 지역적 선형 임베딩 (locally-linear embedding, LLE)
        - 연관 규칙 학습 (association rule learning)
- 준지도 학습
    - 어떤 알고리즘은 일부만 레이블이 있는 데이터를 다룰 수 있다.
    - 지도 학습과 비지도 학습의 조합으로 이루어져 있다.
- 강화 학습
    - 학습하는 시스템을 에이전트라고 부르며, 환경을 관찰해서 행동을 실행하고 그 결과로 보상(또는 벌점)을 받는다.

            

#
### 차원 축소 (demensionality reduction)
- 너무 많은 정보를 잃지 않으면서 데이터를 간소화
- 이렇게 하는 한 가지 방법은 특성 추출(feature extraction)을 한다.
    - 상관관계가 있는 여러 특성을 하나로 합치는 것
    - 예를 들어 차의 주행거리는 연식과 강하게 연관되어 있으므로 차원 축소 알고리즘으로 두 특성을 차의 마모 정도를 나타내는 하나의 특성으로 합칠 수 있다.

#

### 1.4.2 배치 학습과 온라인 학습
### 배치 학습 (batch learning)
- 시스템이 점진적으로 학습할 수 없다. 가용한 데이터를 모두 사용해 훈련한다,
- 오프라인 학습(offline learning)
- 간단하고 잘 작동하지만, 전체 데이터셋을 사용해 훈련하는 데 몇 시간이 소요될 수 있다.

### 온라인 학습 (online learning)
- 데이터를 순차적으로 한 개씩 또는 미니배치(mini-batch)라 부르는 작은 묶음 단위로 주입하여 시스템을 훈련시킨다.
- 매 학습 단계가 빠르고 비용이 적게 들어 시스템은 데이터가 도착하는 대로 즉시 학습할 수 있다.

#
### 1.4.3 사례 기반 학습과 모델 기반 학습
### 사례 기반 학습
- 유사도 측정을 사용해 새로운 데이터와 학습한 샘플을 비교하는 식으로 일반화한다
### 모델 기반 학습
- 샘플들의 모델을 만들어 예측(prediction)에 사용하는 것
#

### 과대적합 (overfitting)
- 모델이 훈련 데이터에 너무 잘 맞지만, 일반성이 떨어진다는 뜻
- 훈련 데이터에 있는 잡음의 양에 비해 모델이 너무 복잡할 때 발생한다.
- 규제는 과대적합의 위험을 감소시킨다.

### 과대적합의 해결 방법?
- 파라미터 수가 적은 모델을 선택하거나(예를 들면 고차원 다항 모델보다 선형 모델), 훈련 데이터에 있는 특성 수를 줄이거나, 모델에 제약을 가하여 단순화시킨다.
- 훈련 데이터를 더 많이 모은다,
- 훈련 데이터의 잡음을 줄인다. (예를 들면 오류 데이터 수정과 이상치 제거)
#
### 과소적합 (underfitting)
- 모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못할 때 발생

### 과소적합 해결 방법?
- 모델 파라미터가 더 많은 강력한 모델을 선택
- 학습 알고리즘에 더 많은 특성을 제공
- 모델의 제약을 줄인다. (예를 들면 규제 하이퍼파라미터를 감소시킨다.)